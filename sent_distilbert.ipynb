{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bc0109-9953-4d52-8688-5d9a1d8ff850",
   "metadata": {},
   "source": [
    "# Using DistilBert for SA on the Sentiment140 Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db09150-36de-49d3-8af4-d2e4d5c82e56",
   "metadata": {},
   "source": [
    "Import Statements - also, setting random seed for reproducibility and some plot settings for seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9ea11b-567f-47fa-984c-499ec1aac06a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#DistilBert + Tokenizer\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "#train/test/dev split and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2345883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random seed + styling\n",
    "\n",
    "SEED=0\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "plt.style.use(\"classic\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
    "\n",
    "#tqdm progress bar for pandas methods\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03e983e-3991-4b9b-9528-0ba014a9fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c09602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#check if using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b37864-909b-456a-8c04-9739a48a3980",
   "metadata": {},
   "source": [
    "### Final Preprocessing\n",
    "\n",
    "As we will be training this model (as opposed to using an out-of-the-box solution as seen in sent_flair.ipynb, sent_nltk.ipynb, etc.), there will be a few extra steps in regards to preprocessing:\n",
    "\n",
    "Loading dataset, splitting into train, val and test, tokenizing with DistilBert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426835d6-48fd-4974-92df-1fee543fc0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_clean.csv', sep='\\t', usecols=['sent', 'text', 'data_len', 'token_lens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e172451-c2f0-4336-98d2-a29395b461fd",
   "metadata": {},
   "source": [
    "Splitting the dataframe into test, val and test sets.  Test is 0.7, Val and Test are both 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f49afd4-57ca-4aa8-a25b-ef4fd77ea189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, temp = train_test_split(df, test_size=0.3, random_state=SEED)\n",
    "df_val, df_test = train_test_split(temp, test_size=0.5, random_state=SEED )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1942354-d349-4f88-8f4f-cdd975350ae7",
   "metadata": {},
   "source": [
    "Confirming that we properly split data by looking at the shapes of the new datasets.  Also head of train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0fa7e8c-1dc6-4b56-8bf3-ce813028d98a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078253, 4)\n",
      "(231054, 4)\n",
      "(231055, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89f92d",
   "metadata": {},
   "source": [
    "Creating y_train, y_val and y_test from the ['sent'] column of their respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248e1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['sent']\n",
    "y_val = df_val['sent']\n",
    "y_test = df_test['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524e24ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339633d-d100-4b0e-a6e7-9da963947769",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5453f87-73a6-48b9-92f7-b5ecddba8cdc",
   "metadata": {},
   "source": [
    "Finally, the various preprocessing steps are over.\n",
    "The code below:\n",
    "1) Initializes the DistilBert tokenizer and defines the tokenize() function\n",
    "2) Tokenizes the train, val, and test data in turn\n",
    "3) Trains the DistilBert model on the train and val data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09763762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thel0\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9d23b3-2203-41a1-adfb-b582e59a3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize(data,max_len=MAX_LEN) :\n",
    "\n",
    "    input_ids = []\n",
    "\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            data[i],\n",
    "            add_special_tokens=True,\n",
    "            max_length = max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "        \n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892ebaf-64eb-48f9-8c95-f518724826b1",
   "metadata": {},
   "source": [
    "Creating arrays containing input_ids and attention masks as returned by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2c9d56-ef95-4a3b-933a-0cb9a7e0d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 245859/1078253 [01:07<03:47, 3654.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_input_ids, train_attention_masks \u001b[39m=\u001b[39m tokenize(df_train\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m      2\u001b[0m val_input_ids, val_attention_masks \u001b[39m=\u001b[39m tokenize(df_val\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m      3\u001b[0m test_input_ids, test_attention_masks \u001b[39m=\u001b[39m tokenize(df_test\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mvalues)\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(data, max_len)\u001b[0m\n\u001b[0;32m      7\u001b[0m attention_masks \u001b[39m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data))):\n\u001b[1;32m---> 11\u001b[0m     encoded \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[0;32m     12\u001b[0m         data[i],\n\u001b[0;32m     13\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     14\u001b[0m         max_length \u001b[39m=\u001b[39;49m max_len,\n\u001b[0;32m     15\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     16\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     input_ids\u001b[39m.\u001b[39mappend(encoded[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     20\u001b[0m     attention_masks\u001b[39m.\u001b[39mappend(encoded[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2756\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2746\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2747\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2748\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2749\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2753\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2754\u001b[0m )\n\u001b[1;32m-> 2756\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2757\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2758\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2759\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2760\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2761\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2762\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2763\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2764\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2765\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2766\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2767\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2768\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2769\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2770\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2771\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2772\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2773\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2774\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2775\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[0;32m    653\u001b[0m     first_ids,\n\u001b[0;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    669\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[0;32m    615\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 616\u001b[0m         tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    617\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    618\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m         tokenized_text\u001b[39m.\u001b[39mappend(token)\n\u001b[0;32m    546\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m         tokenized_text\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(token))\n\u001b[0;32m    548\u001b[0m \u001b[39m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py:201\u001b[0m, in \u001b[0;36mDistilBertTokenizer._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    199\u001b[0m split_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 201\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbasic_tokenizer\u001b[39m.\u001b[39;49mtokenize(text, never_split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens):\n\u001b[0;32m    202\u001b[0m         \u001b[39m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbasic_tokenizer\u001b[39m.\u001b[39mnever_split:\n\u001b[0;32m    204\u001b[0m             split_tokens\u001b[39m.\u001b[39mappend(token)\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py:407\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrip_accents:\n\u001b[0;32m    406\u001b[0m             token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_strip_accents(token)\n\u001b[1;32m--> 407\u001b[0m     split_tokens\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_split_on_punc(token, never_split))\n\u001b[0;32m    409\u001b[0m output_tokens \u001b[39m=\u001b[39m whitespace_tokenize(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(split_tokens))\n\u001b[0;32m    410\u001b[0m \u001b[39mreturn\u001b[39;00m output_tokens\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py:433\u001b[0m, in \u001b[0;36mBasicTokenizer._run_split_on_punc\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(chars):\n\u001b[0;32m    432\u001b[0m     char \u001b[39m=\u001b[39m chars[i]\n\u001b[1;32m--> 433\u001b[0m     \u001b[39mif\u001b[39;00m _is_punctuation(char):\n\u001b[0;32m    434\u001b[0m         output\u001b[39m.\u001b[39mappend([char])\n\u001b[0;32m    435\u001b[0m         start_new_word \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py:294\u001b[0m, in \u001b[0;36m_is_punctuation\u001b[1;34m(char)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_punctuation\u001b[39m(char):\n\u001b[0;32m    293\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Checks whether `char` is a punctuation character.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m     cp \u001b[39m=\u001b[39m \u001b[39mord\u001b[39;49m(char)\n\u001b[0;32m    295\u001b[0m     \u001b[39m# We treat all non-letter/number ASCII as punctuation.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m     \u001b[39m# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     \u001b[39m# Punctuation class but we treat them as punctuation anyways, for\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[39m# consistency.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m (cp \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m33\u001b[39m \u001b[39mand\u001b[39;00m cp \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m47\u001b[39m) \u001b[39mor\u001b[39;00m (cp \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m58\u001b[39m \u001b[39mand\u001b[39;00m cp \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m64\u001b[39m) \u001b[39mor\u001b[39;00m (cp \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m91\u001b[39m \u001b[39mand\u001b[39;00m cp \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m96\u001b[39m) \u001b[39mor\u001b[39;00m (cp \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m123\u001b[39m \u001b[39mand\u001b[39;00m cp \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m126\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_input_ids, train_attention_masks = tokenize(df_train.text.values)\n",
    "val_input_ids, val_attention_masks = tokenize(df_val.text.values)\n",
    "test_input_ids, test_attention_masks = tokenize(df_test.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2db257-388a-471d-93e6-f7ff1caba9e4",
   "metadata": {},
   "source": [
    "Model creation - setting the optimizer, loss func, and accuracy metric.  Model comprises of two input layers, one taking the input_ids, the other the corresponding attention mask.  These are fed into the bertlike model - which in this case DistilBert.  Take the hidden state of the cls token as a representation of the sentences sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1f058-f5e9-46b2-82f2-9bd5991f54d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5 #1e-4, 3e-4, 5e-5, 3e-5\n",
    "N_EPOCHS = 3\n",
    "BATCH_SIZE = 64 #8, 16, 32, 64, 128\n",
    "\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ec565",
   "metadata": {},
   "source": [
    "After the model creation above, we display the models summary.  We can see everything seems to be connected up nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e57e5c",
   "metadata": {},
   "source": [
    "Here is the portion that takes by far the longest - model training.\n",
    "\n",
    "With 10gb of VRAM on my home machine, each epoch (training data is ~1 million tweets) takes around 90 minutes.  So training anywhere from 2 to 4 epochs, expect training times of around 3-6 hours with a similar machine.\n",
    "\n",
    "This should be less of a problem if running on better hardware or through a cloud platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 1751/16848 [==>...........................] - ETA: 1:21:01 - loss: 0.4185 - accuracy: 0.8084"
     ]
    }
   ],
   "source": [
    "model.fit([train_input_ids, train_attention_masks], y_train, validation_data=([val_input_ids, val_attention_masks], y_val), batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ce90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to save model\n",
    "\n",
    "model.save_pretrained('trained_distilbert_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9449fd",
   "metadata": {},
   "source": [
    "Below is the final evaluation of the model on `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2f0db-df90-4ad5-8e11-8ca71cd93c13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'}), <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate([test_input_ids,test_attention_masks], y_test, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'}), <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "results = model.evaluate([test_input_ids,test_attention_masks], y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37611e1b-aeef-4248-a6af-042a8e2285c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36584365367889404, 0.8491830825805664]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed216af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to save df_test\n",
    "\n",
    "df_test.to_csv('data_test.csv', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
